{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "# with open('creds.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     USERNAME = data['user']\n",
    "#     PASSWORD = data['password']\n",
    "#     SF_ACCOUNT = data['account']\n",
    "#     SF_WH = data['warehouse']\n",
    "\n",
    "# CONNECTION_PARAMETERS = {\n",
    "#    \"account\": SF_ACCOUNT,\n",
    "#    \"user\": USERNAME,\n",
    "#    \"password\": PASSWORD,\n",
    "# }\n",
    "\n",
    "# session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : ESTPEGION\n",
      "Role                        : ACCOUNTADMIN\n",
      "Database                    : KPMG\n",
      "Schema                      : PUBLIC\n",
      "Warehouse                   : COMPUTE_WH\n",
      "Snowflake version           : 7.6.4\n",
      "Snowpark for Python version : 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 此处开始全部为在snowflake整合数据，无需重复执行；已注释\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SNOWFLAKE_SAMPLE_DATA already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.sql('''create database if not exists snowflake_sample_data from share sfc_samples.sample_data''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "# session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "# session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='3X-LARGE'\").collect()\n",
    "# session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n",
    "# session.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n",
    "# session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either 100 or 10 for the TPC-DS Dataset size to use below. See (https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html)[here] for more information If you choose 100, I recommend >= 3XL warehouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPCDS_SIZE_PARAM = 100\n",
    "# SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' # Name of Snowflake Sample Database might be different...\n",
    "\n",
    "# if TPCDS_SIZE_PARAM == 100: \n",
    "#     TPCDS_SCHEMA = 'TPCDS_SF100TCL'\n",
    "# elif TPCDS_SIZE_PARAM == 10:\n",
    "#     TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "# else:\n",
    "#     raise ValueError(\"Invalid TPCDS_SIZE_PARAM selection\")\n",
    "    \n",
    "# store_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store_sales')\n",
    "# catalog_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.catalog_sales') \n",
    "# web_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.web_sales') \n",
    "# date = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.date_dim')\n",
    "# dim_stores = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store')\n",
    "# customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "# address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "# demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will aggregate sales by customer across all channels(web, store, catalogue) and join that to customer demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_sales_agged = store_sales.group_by('ss_customer_sk').agg(F.sum('ss_sales_price').as_('total_sales'))\n",
    "# web_sales_agged = web_sales.group_by('ws_bill_customer_sk').agg(F.sum('ws_sales_price').as_('total_sales'))\n",
    "# catalog_sales_agged = catalog_sales.group_by('cs_bill_customer_sk').agg(F.sum('cs_sales_price').as_('total_sales'))\n",
    "# store_sales_agged = store_sales_agged.rename('ss_customer_sk', 'customer_sk')\n",
    "# web_sales_agged = web_sales_agged.rename('ws_bill_customer_sk', 'customer_sk')\n",
    "# catalog_sales_agged = catalog_sales_agged.rename('cs_bill_customer_sk', 'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sales = store_sales_agged.union_all(web_sales_agged)\n",
    "# total_sales = total_sales.union_all(catalog_sales_agged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_sales = total_sales.group_by('customer_sk').agg(F.sum('total_sales').as_('total_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer = customer.select('c_customer_sk','c_current_hdemo_sk', 'c_current_addr_sk', 'c_customer_id', 'c_birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_SK\"  |\"C_CURRENT_HDEMO_SK\"  |\"C_CURRENT_ADDR_SK\"  |\"C_CUSTOMER_ID\"   |\"C_BIRTH_YEAR\"  |\"CA_ADDRESS_SK\"  |\"CA_ZIP\"  |\"CD_DEMO_SK\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_CREDIT_RATING\"  |\"CD_EDUCATION_STATUS\"  |\"CD_DEP_COUNT\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|3542785        |86                    |46344452             |AAAAAAAABAPAGDAA  |1977            |46344452         |40587     |86            |F            |D                    |Good                |Secondary              |0               |\n",
      "|3542872        |1327                  |21821594             |AAAAAAAAIFPAGDAA  |1976            |21821594         |48828     |1327          |M            |W                    |Good                |Unknown                |0               |\n",
      "|3543309        |1686                  |30523963             |AAAAAAAANABBGDAA  |1929            |30523963         |46192     |1686          |F            |D                    |Low Risk            |Primary                |0               |\n",
      "|3545057        |2867                  |15782315             |AAAAAAAABOHBGDAA  |1984            |15782315         |12808     |2867          |M            |W                    |High Risk           |Unknown                |0               |\n",
      "|3545891        |5798                  |6442848              |AAAAAAAADCLBGDAA  |1944            |6442848          |30191     |5798          |F            |W                    |Good                |Advanced Degree        |1               |\n",
      "|3546763        |5250                  |14908351             |AAAAAAAALIOBGDAA  |1939            |14908351         |36871     |5250          |F            |U                    |Unknown             |Unknown                |0               |\n",
      "|3547178        |3774                  |32208899             |AAAAAAAAKCACGDAA  |1943            |32208899         |58054     |3774          |F            |S                    |High Risk           |Unknown                |0               |\n",
      "|3548015        |4361                  |40496                |AAAAAAAAPGDCGDAA  |1948            |40496            |69275     |4361          |M            |M                    |Unknown             |College                |0               |\n",
      "|3548193        |7119                  |11054695             |AAAAAAAABCECGDAA  |1985            |11054695         |70001     |7119          |M            |U                    |Low Risk            |4 yr Degree            |1               |\n",
      "|3548908        |3510                  |12857019             |AAAAAAAAMOGCGDAA  |1951            |12857019         |48054     |3510          |F            |U                    |High Risk           |Primary                |0               |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# customer = customer.join(address.select('ca_address_sk', 'ca_zip'), customer['c_current_addr_sk'] == address['ca_address_sk'] )\n",
    "# customer = customer.join(demo.select('cd_demo_sk', 'cd_gender', 'cd_marital_status', 'cd_credit_rating', 'cd_education_status', 'cd_dep_count'),\n",
    "#                                 customer['c_current_hdemo_sk'] == demo['cd_demo_sk'] )\n",
    "# customer = customer.rename('c_customer_sk', 'customer_sk')\n",
    "# customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = total_sales.join(customer, on='customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.use_database('tpcds_xgboost')\n",
    "# session.use_schema('demo')\n",
    "# final_df.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS successfully created.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.sql('CREATE OR REPLACE STAGE ml_models ').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始执行\n",
    "## Machine Learning Model Building/Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models\",overwrite=True)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sproc\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "#train_model_sp = F.sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models\")\n",
    "train_model_sp = sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "#train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "session.add_import(\"@ml_models/model.joblib\")  \n",
    "\n",
    "features = [ 'C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location='@ml_models', replace=True, name=\"clv_xgboost_udf\")\n",
    "def predict(df:  T.PandasDataFrame[int, str, str, str, str, str, int]) -> T.PandasSeries[float]:\n",
    "       m = read_file('model.joblib')       \n",
    "       df.columns = features\n",
    "       return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = session.table('feature_store')\n",
    "inference_df = inference_df.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "inputs = inference_df.drop(\"TOTAL_SALES\")\n",
    "snowdf_results = inference_df.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('TOTAL_SALES')).alias('ACTUAL_SALES')\n",
    "                    )\n",
    "snowdf_results.write.mode('overwrite').save_as_table('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "353961104846001ffa111d7d98923933ef13c251c8e9b3ebc563f652eb6b45f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
