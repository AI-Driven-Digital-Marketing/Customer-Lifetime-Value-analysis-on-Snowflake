{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : SUCKFLAKE\n",
      "Role                        : ACCOUNTADMIN\n",
      "Database                    : None\n",
      "Schema                      : None\n",
      "Warehouse                   : COMPUTE_WH\n",
      "Snowflake version           : 7.9.0\n",
      "Snowpark for Python version : 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SNOWFLAKE_SAMPLE_DATA already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('''create database if not exists snowflake_sample_data from share sfc_samples.sample_data''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='3X-LARGE'\").collect()\n",
    "session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n",
    "session.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either 100 or 10 for the TPC-DS Dataset size to use below. See (https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html)[here] for more information If you choose 100, I recommend >= 3XL warehouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPCDS_SIZE_PARAM = 10\n",
    "SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' # Name of Snowflake Sample Database might be different...\n",
    "\n",
    "if TPCDS_SIZE_PARAM == 100: \n",
    "    TPCDS_SCHEMA = 'TPCDS_SF100TCL'\n",
    "elif TPCDS_SIZE_PARAM == 10:\n",
    "    TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "else:\n",
    "    raise ValueError(\"Invalid TPCDS_SIZE_PARAM selection\")\n",
    "    \n",
    "store_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store_sales')\n",
    "catalog_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.catalog_sales') \n",
    "web_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.web_sales') \n",
    "date = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.date_dim')\n",
    "dim_stores = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store')\n",
    "customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((store_sales.count(), len(store_sales.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will aggregate sales by customer across all channels(web, store, catalogue) and join that to customer demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_agged = store_sales.group_by('ss_customer_sk').agg(F.sum('ss_sales_price').as_('total_sales'))\n",
    "web_sales_agged = web_sales.group_by('ws_bill_customer_sk').agg(F.sum('ws_sales_price').as_('total_sales'))\n",
    "catalog_sales_agged = catalog_sales.group_by('cs_bill_customer_sk').agg(F.sum('cs_sales_price').as_('total_sales'))\n",
    "store_sales_agged = store_sales_agged.rename('ss_customer_sk', 'customer_sk')\n",
    "web_sales_agged = web_sales_agged.rename('ws_bill_customer_sk', 'customer_sk')\n",
    "catalog_sales_agged = catalog_sales_agged.rename('cs_bill_customer_sk', 'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = store_sales_agged.union_all(web_sales_agged)\n",
    "total_sales = total_sales.union_all(catalog_sales_agged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = total_sales.group_by('customer_sk').agg(F.sum('total_sales').as_('total_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.select('c_customer_sk','c_current_hdemo_sk', 'c_current_addr_sk', 'c_customer_id', 'c_birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.join(address.select('ca_address_sk', 'ca_zip'), customer['c_current_addr_sk'] == address['ca_address_sk'] )\n",
    "customer = customer.join(demo.select('cd_demo_sk', 'cd_gender', 'cd_marital_status', 'cd_credit_rating', 'cd_education_status', 'cd_dep_count'),\n",
    "                                customer['c_current_hdemo_sk'] == demo['cd_demo_sk'] )\n",
    "customer = customer.rename('c_customer_sk', 'customer_sk')\n",
    "#customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = total_sales.join(customer, on='customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "final_df.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS_10T successfully created.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE ml_models_10T ').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS_LR_10T successfully created.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create stage for LR\n",
    "session.sql('CREATE OR REPLACE STAGE ml_models_LR_10T ').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "In this part, we will training a XGBoost model and upload to snowflake stage. This will create `stored procedure` on snowflake, and we can call this model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric, test_metric = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> list:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    \n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    train_preds = pipe.predict(train_x)\n",
    "    #rmse = mean_squared_error(test_y, test_preds)\n",
    "    train_metric, test_metric = {}, {}\n",
    "    train_metric['MAPE'] = mean_absolute_percentage_error(train_y, train_preds)\n",
    "    train_metric['RMSE'] = mean_absolute_error(train_y, train_preds)\n",
    "    train_metric['R2'] = r2_score(train_y, train_preds)\n",
    "    \n",
    "    test_metric['MAPE'] =  mean_absolute_percentage_error(test_y, test_preds)\n",
    "    test_metric['RMSE'] =  mean_absolute_error(test_y, test_preds)\n",
    "    test_metric['R2'] =  r2_score(test_y, test_preds)\n",
    "    \n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models_10T\",overwrite=True)\n",
    "    print('successes')\n",
    "    return ['Training',train_metric,'Test', test_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse('snowpark_opt_wh')\n",
    "train_model_sp = F.sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models_10T\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "metrics = train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Training\",\n",
      "  {\n",
      "    \"MAPE\": 0.10485570980437563,\n",
      "    \"R2\": 0.0002112945511688613,\n",
      "    \"RMSE\": 3648.1393828479954\n",
      "  },\n",
      "  \"Test\",\n",
      "  {\n",
      "    \"MAPE\": 0.10488914530523744,\n",
      "    \"R2\": -3.706236328282486e-05,\n",
      "    \"RMSE\": 3649.615726110742\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model_LR(session: snowflake.snowpark.Session) -> list:\n",
    "    \n",
    "#---------Preprocess Start ---------\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "#---------Preprocess End---------\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('LinearRegression', LinearRegression())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    train_preds = pipe.predict(train_x)\n",
    "    #rmse = mean_squared_error(test_y, test_preds)\n",
    "    train_metric, test_metric = {}, {}\n",
    "    train_metric['MAPE'] = mean_absolute_percentage_error(train_y, train_preds)\n",
    "    train_metric['RMSE'] = mean_absolute_error(train_y, train_preds)\n",
    "    train_metric['R2'] = r2_score(train_y, train_preds)\n",
    "    \n",
    "    test_metric['MAPE'] =  mean_absolute_percentage_error(test_y, test_preds)\n",
    "    test_metric['RMSE'] =  mean_absolute_error(test_y, test_preds)\n",
    "    test_metric['R2'] =  r2_score(test_y, test_preds)\n",
    "    model_file = os.path.join('/tmp', 'model_LR.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models_LR_10T\",overwrite=True)\n",
    "    print('successes')\n",
    "    return ['Training',train_metric,'Test', test_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----USE DATABASE AND SESSION\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')\n",
    "#Create stage for LR\n",
    "session.sql('CREATE OR REPLACE STAGE ml_models_LR_10T ').collect()\n",
    "\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "train_model_sp = F.sproc(train_model_LR, session=session, replace=True, is_permanent=True, name=\"LinearRegresson_sproc\", stage_location=\"@ml_models_LR_10T\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "LR_metric = train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Training\",\n",
      "  {\n",
      "    \"MAPE\": 0.10487565167778798,\n",
      "    \"R2\": 0.00018409705548227961,\n",
      "    \"RMSE\": 3648.7580637718493\n",
      "  },\n",
      "  \"Test\",\n",
      "  {\n",
      "    \"MAPE\": 0.10488045441859324,\n",
      "    \"R2\": -0.00018483839087624077,\n",
      "    \"RMSE\": 3649.308301941748\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(LR_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Call `stored procedure` or say our `model` is billed by predict times, \\$0.52 for 10T model, \\\\$1.3 for 100T model.\n",
    "- XGBoost model in stage `@ml_models_10T` is trained on 10T model and called `model.joblib`.\n",
    "- Linear regression in stage `ml_models_LR_10T` is trained on 10T model and called `model_LR.joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')\n",
    "features = [ 'C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "session.add_import(\"@ml_models_10T/model.joblib\")  \n",
    "session.add_import(\"@ml_models_LR_10T/model_LR.joblib\")\n",
    "features = [ 'C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "# choose model here\n",
    "stage_name = 'ml_models_10T'\n",
    "model_name = 'model.joblib'\n",
    "\n",
    "# stage_name = 'ml_models_LR_10T'\n",
    "# model_name = 'model_LR.joblib' \n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "    import os, joblib\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location=f'@{stage_name}', replace=True, name=\"clv_xgboost_udf\")\n",
    "def predict(df:  T.PandasDataFrame[int, str, str, str, str, str, int]) -> T.PandasSeries[float]:\n",
    "    m = read_file(model_name)        \n",
    "    df.columns = features\n",
    "    return m.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on data from feature_store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take all training dataset as input, let use smaller dataset in the following\n",
    "# inference_df = session.table('feature_store').limit(100)\n",
    "# inference_df = inference_df.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "# inputs = inference_df.drop(\"TOTAL_SALES\")\n",
    "# snowdf_results = inference_df.select(*inputs,\n",
    "#                     predict(*inputs).alias('PREDICTION'), \n",
    "#                     (F.col('TOTAL_SALES')).alias('ACTUAL_SALES')\n",
    "#                     )\n",
    "# snowdf_results.write.mode('overwrite').save_as_table('predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAPE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "|\"COUNT(*)\"  |\n",
      "--------------\n",
      "|50178278    |\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('select count(*) from tpcds_xgboost.demo.tpc_TRAIN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "|\"COUNT(*)\"  |\n",
      "--------------\n",
      "|12548711    |\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('select count(*) from tpcds_xgboost.demo.tpc_TEST').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on typed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume this is from strealit UI\n",
    "typed_input = [[1969, '66060','M','U','Low Risk','2 yr Degree', 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = session.create_dataframe(typed_input, schema=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "typed_output = input_df.select(*input_df,\n",
    "                    predict(*input_df).alias('PREDICTION'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(typed_output.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_BIRTH_YEAR</th>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA_ZIP</th>\n",
       "      <td>66060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD_GENDER</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD_MARITAL_STATUS</th>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD_CREDIT_RATING</th>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD_EDUCATION_STATUS</th>\n",
       "      <td>2 yr Degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD_DEP_COUNT</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREDICTION</th>\n",
       "      <td>32337.777344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 \n",
       "C_BIRTH_YEAR                 1969\n",
       "CA_ZIP                      66060\n",
       "CD_GENDER                       M\n",
       "CD_MARITAL_STATUS               U\n",
       "CD_CREDIT_RATING         Low Risk\n",
       "CD_EDUCATION_STATUS   2 yr Degree\n",
       "CD_DEP_COUNT                    1\n",
       "PREDICTION           32337.777344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = temp.T\n",
    "temp1.columns = ['']\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to execute query [queryID: 01ab04bd-0004-5eaf-0052-d38700020256]  SELECT \"C_BIRTH_YEAR\", \"CA_ZIP\", \"CD_GENDER\", \"CD_MARITAL_STATUS\", \"CD_CREDIT_RATING\", \"CD_EDUCATION_STATUS\", \"CD_DEP_COUNT\", clv_xgboost_udf(\"C_BIRTH_YEAR\", \"CA_ZIP\", \"CD_GENDER\", \"CD_MARITAL_STATUS\", \"CD_CREDIT_RATING\", \"CD_EDUCATION_STATUS\", \"CD_DEP_COUNT\") AS \"PREDICTION\" FROM ( SELECT $1 AS \"C_BIRTH_YEAR\", $2 AS \"CA_ZIP\", $3 AS \"CD_GENDER\", $4 AS \"CD_MARITAL_STATUS\", $5 AS \"CD_CREDIT_RATING\", $6 AS \"CD_EDUCATION_STATUS\", $7 AS \"CD_DEP_COUNT\" FROM  VALUES (1969 :: INT, '66060' :: STRING, 'M' :: STRING, 'U' :: STRING, 'Low Risk' :: STRING, '2 yr Degree' :: STRING, 1 :: INT))\n",
      "100357 (P0000): Python Interpreter Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 49, in compute\n",
      "    return lock_function_once(func, invoked)(df)\n",
      "  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 38, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 24, in predict\n",
      "  File \"C:\\Users\\zwdua\\anaconda3\\envs\\snowpark\\lib\\site-packages\\cachetools\\decorators.py\", line 26, in wrapper\n",
      "  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 19, in read_file\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 577, in load\n",
      "    obj = _unpickle(fobj)\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 506, in _unpickle\n",
      "    obj = unpickler.load()\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1212, in load\n",
      "    dispatch[key[0]](self)\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 331, in load_build\n",
      "    Unpickler.load_build(self)\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1705, in load_build\n",
      "    setstate(state)\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 1451, in __setstate__\n",
      "    _check_call(\n",
      "  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 218, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [19:05:50] /tmp/abs_3fs5_ko2hu/croots/recipe/xgboost-split_1659548945693/work/src/common/json.cc:458: Expecting: \"\"\", got: \"76 \", around character position: 1\n",
      "    {L\\0\\0\\0\\0\\0\\0\\0\n",
      "    ^~~~~~~~~\n",
      "Stack trace:\n",
      "  [bt] (0) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x9dbf0) [0xffff7c420bf0]\n",
      "  [bt] (1) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const+0xadc) [0xffff7c45043c]\n",
      "  [bt] (2) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0xd4b10) [0xffff7c457b10]\n",
      "  [bt] (3) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::ParseObject()+0x304) [0xffff7c4553c4]\n",
      "  [bt] (4) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Parse()+0x178) [0xffff7c450dd8]\n",
      "  [bt] (5) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Load()+0x30) [0xffff7c450f00]\n",
      "  [bt] (6) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::Json::Load(xgboost::StringView)+0x4c) [0xffff7c450f8c]\n",
      "  [bt] (7) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x1d2038) [0xffff7c555038]\n",
      "  [bt] (8) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(XGBoosterUnserializeFromBuffer+0x60) [0xffff7c40ca40]\n",
      "\n",
      "\n",
      " in function CLV_XGBOOST_UDF with handler udf_py_1835141308.compute\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01ab04bd-0004-5eaf-0052-d38700020256: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 49, in compute\n    return lock_function_once(func, invoked)(df)\n  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 38, in wrapper\n    result = f(*args, **kwargs)\n  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 24, in predict\n  File \"C:\\Users\\zwdua\\anaconda3\\envs\\snowpark\\lib\\site-packages\\cachetools\\decorators.py\", line 26, in wrapper\n  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 19, in read_file\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 577, in load\n    obj = _unpickle(fobj)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 506, in _unpickle\n    obj = unpickler.load()\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1212, in load\n    dispatch[key[0]](self)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 331, in load_build\n    Unpickler.load_build(self)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1705, in load_build\n    setstate(state)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 1451, in __setstate__\n    _check_call(\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 218, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [19:05:50] /tmp/abs_3fs5_ko2hu/croots/recipe/xgboost-split_1659548945693/work/src/common/json.cc:458: Expecting: \"\"\", got: \"76 \", around character position: 1\n    {L\\0\\0\\0\\0\\0\\0\\0\n    ^~~~~~~~~\nStack trace:\n  [bt] (0) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x9dbf0) [0xffff7c420bf0]\n  [bt] (1) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const+0xadc) [0xffff7c45043c]\n  [bt] (2) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0xd4b10) [0xffff7c457b10]\n  [bt] (3) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::ParseObject()+0x304) [0xffff7c4553c4]\n  [bt] (4) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Parse()+0x178) [0xffff7c450dd8]\n  [bt] (5) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Load()+0x30) [0xffff7c450f00]\n  [bt] (6) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::Json::Load(xgboost::StringView)+0x4c) [0xffff7c450f8c]\n  [bt] (7) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x1d2038) [0xffff7c555038]\n  [bt] (8) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(XGBoosterUnserializeFromBuffer+0x60) [0xffff7c40ca40]\n\n\n in function CLV_XGBOOST_UDF with handler udf_py_1835141308.compute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtyped_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:138\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[1;32m--> 138\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     plan \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_plan\n\u001b[0;32m    140\u001b[0m     api_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39mplan\u001b[38;5;241m.\u001b[39mapi_calls,\n\u001b[0;32m    142\u001b[0m         {TelemetryField\u001b[38;5;241m.\u001b[39mNAME\u001b[38;5;241m.\u001b[39mvalue: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    143\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:546\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self, statement_params, block)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;129m@df_collect_api_telemetry\u001b[39m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, statement_params: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, block: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    533\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[List[Row], AsyncJob]:\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the query representing this DataFrame and returns the result as a\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03m    list of :class:`Row` objects.\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m        :meth:`collect_nowait()`\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:581\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[1;34m(self, statement_params, block, data_type)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:406\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_in_stored_procedure() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsync query is not supported in stored procedure yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m     )\n\u001b[1;32m--> 406\u001b[0m result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:156\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    154\u001b[0m         e\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:89\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m         tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:494\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m holder, id_ \u001b[38;5;129;01min\u001b[39;00m placeholders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    493\u001b[0m     final_query \u001b[38;5;241m=\u001b[39m final_query\u001b[38;5;241m.\u001b[39mreplace(holder, id_)\n\u001b[1;32m--> 494\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    505\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    507\u001b[0m result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mdescription\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:104\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    101\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:98\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    101\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[0;32m    102\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:333\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m     query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:317\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_statement_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m--> 317\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[0;32m    319\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\connector\\cursor.py:804\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[0;32m    800\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    801\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m    803\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[1;32m--> 804\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:276\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    255\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    284\u001b[0m             error_class,\n\u001b[0;32m    285\u001b[0m             error_value,\n\u001b[0;32m    286\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:331\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 331\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_errorhandler\u001b[39m(\n\u001b[0;32m    194\u001b[0m     connection: SnowflakeConnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m        A Snowflake error.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[0;32m    211\u001b[0m         msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m         errno\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    213\u001b[0m         sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m         sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    215\u001b[0m         done_format_msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    216\u001b[0m         connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[0;32m    217\u001b[0m         cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mSnowparkSQLException\u001b[0m: (1304): 01ab04bd-0004-5eaf-0052-d38700020256: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 49, in compute\n    return lock_function_once(func, invoked)(df)\n  File \"/home/udf/1389594464/udf_py_1835141308.zip/udf_py_1835141308.py\", line 38, in wrapper\n    result = f(*args, **kwargs)\n  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 24, in predict\n  File \"C:\\Users\\zwdua\\anaconda3\\envs\\snowpark\\lib\\site-packages\\cachetools\\decorators.py\", line 26, in wrapper\n  File \"C:\\Users\\zwdua\\AppData\\Local\\Temp\\ipykernel_14476\\3989733615.py\", line 19, in read_file\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 577, in load\n    obj = _unpickle(fobj)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 506, in _unpickle\n    obj = unpickler.load()\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1212, in load\n    dispatch[key[0]](self)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 331, in load_build\n    Unpickler.load_build(self)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/pickle.py\", line 1705, in load_build\n    setstate(state)\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 1451, in __setstate__\n    _check_call(\n  File \"/usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/python3.8/site-packages/xgboost/core.py\", line 218, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [19:05:50] /tmp/abs_3fs5_ko2hu/croots/recipe/xgboost-split_1659548945693/work/src/common/json.cc:458: Expecting: \"\"\", got: \"76 \", around character position: 1\n    {L\\0\\0\\0\\0\\0\\0\\0\n    ^~~~~~~~~\nStack trace:\n  [bt] (0) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x9dbf0) [0xffff7c420bf0]\n  [bt] (1) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const+0xadc) [0xffff7c45043c]\n  [bt] (2) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0xd4b10) [0xffff7c457b10]\n  [bt] (3) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::ParseObject()+0x304) [0xffff7c4553c4]\n  [bt] (4) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Parse()+0x178) [0xffff7c450dd8]\n  [bt] (5) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::JsonReader::Load()+0x30) [0xffff7c450f00]\n  [bt] (6) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(xgboost::Json::Load(xgboost::StringView)+0x4c) [0xffff7c450f8c]\n  [bt] (7) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(+0x1d2038) [0xffff7c555038]\n  [bt] (8) /usr/lib/python_udf/ecd2f7835d1dd3652fa432f40f924b16e72b73e47f2a181f4b0c6bcc9157db9e/lib/libxgboost.so(XGBoosterUnserializeFromBuffer+0x60) [0xffff7c40ca40]\n\n\n in function CLV_XGBOOST_UDF with handler udf_py_1835141308.compute"
     ]
    }
   ],
   "source": [
    "typed_output.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "353961104846001ffa111d7d98923933ef13c251c8e9b3ebc563f652eb6b45f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
