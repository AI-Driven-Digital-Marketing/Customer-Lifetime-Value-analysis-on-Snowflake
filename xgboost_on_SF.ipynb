{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : ZIWEI\n",
      "Role                        : ACCOUNTADMIN\n",
      "Database                    : MYSUPERFANCYBIKEDATA\n",
      "Schema                      : KPMG\n",
      "Warehouse                   : COMPUTE_WH\n",
      "Snowflake version           : 7.6.4\n",
      "Snowpark for Python version : 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection_Ziwei.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SNOWFLAKE_SAMPLE_DATA already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('''create database if not exists snowflake_sample_data from share sfc_samples.sample_data''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='3X-LARGE'\").collect()\n",
    "session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n",
    "session.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either 100 or 10 for the TPC-DS Dataset size to use below. See (https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html)[here] for more information If you choose 100, I recommend >= 3XL warehouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPCDS_SIZE_PARAM = 10\n",
    "SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' # Name of Snowflake Sample Database might be different...\n",
    "\n",
    "if TPCDS_SIZE_PARAM == 100: \n",
    "    TPCDS_SCHEMA = 'TPCDS_SF100TCL'\n",
    "elif TPCDS_SIZE_PARAM == 10:\n",
    "    TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "else:\n",
    "    raise ValueError(\"Invalid TPCDS_SIZE_PARAM selection\")\n",
    "    \n",
    "store_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store_sales')\n",
    "catalog_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.catalog_sales') \n",
    "web_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.web_sales') \n",
    "date = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.date_dim')\n",
    "dim_stores = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store')\n",
    "customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288010550524, 23)\n"
     ]
    }
   ],
   "source": [
    "print((store_sales.count(), len(store_sales.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will aggregate sales by customer across all channels(web, store, catalogue) and join that to customer demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_agged = store_sales.group_by('ss_customer_sk').agg(F.sum('ss_sales_price').as_('total_sales'))\n",
    "web_sales_agged = web_sales.group_by('ws_bill_customer_sk').agg(F.sum('ws_sales_price').as_('total_sales'))\n",
    "catalog_sales_agged = catalog_sales.group_by('cs_bill_customer_sk').agg(F.sum('cs_sales_price').as_('total_sales'))\n",
    "store_sales_agged = store_sales_agged.rename('ss_customer_sk', 'customer_sk')\n",
    "web_sales_agged = web_sales_agged.rename('ws_bill_customer_sk', 'customer_sk')\n",
    "catalog_sales_agged = catalog_sales_agged.rename('cs_bill_customer_sk', 'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = store_sales_agged.union_all(web_sales_agged)\n",
    "total_sales = total_sales.union_all(catalog_sales_agged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = total_sales.group_by('customer_sk').agg(F.sum('total_sales').as_('total_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.select('c_customer_sk','c_current_hdemo_sk', 'c_current_addr_sk', 'c_customer_id', 'c_birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.join(address.select('ca_address_sk', 'ca_zip'), customer['c_current_addr_sk'] == address['ca_address_sk'] )\n",
    "customer = customer.join(demo.select('cd_demo_sk', 'cd_gender', 'cd_marital_status', 'cd_credit_rating', 'cd_education_status', 'cd_dep_count'),\n",
    "                                customer['c_current_hdemo_sk'] == demo['cd_demo_sk'] )\n",
    "customer = customer.rename('c_customer_sk', 'customer_sk')\n",
    "#customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = total_sales.join(customer, on='customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "final_df.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS_10T successfully created.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE ml_models_10T ').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS_LR_10T successfully created.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create stage for LR\n",
    "\n",
    "session.sql('CREATE OR REPLACE STAGE ml_models_LR_10T ').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "In this part, we will training a XGBoost model and upload to snowflake stage. This will create `stored procedure` on snowflake, and we can call this model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models_10T\",overwrite=True)\n",
    "    print('successes')\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34158914.31717292"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.use_warehouse('snowpark_opt_wh')\n",
    "train_model_sp = F.sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models_10T\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model_LR(session: snowflake.snowpark.Session) -> float:\n",
    "    \n",
    "#---------Preprocess Start ---------\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "#---------Preprocess End---------\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('LinearRegression', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models_LR_10T\",overwrite=True)\n",
    "    print('successes')\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34179024.05535148"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----USE DATABASE AND SESSION\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')\n",
    "#Create stage for LR\n",
    "session.sql('CREATE OR REPLACE STAGE ml_models_LR_10T ').collect()\n",
    "\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "train_model_sp = F.sproc(train_model_LR, session=session, replace=True, is_permanent=True, name=\"LinearRegresson_sproc\", stage_location=\"@ml_models_LR_10T\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Call `stored procedure` or say our `model` is billed by predict times, \\$0.52 for 10T model, \\\\$1.3 for 100T model.\n",
    "- `@ml_models` is 100T model\n",
    "- `@ml_models_10T` is 10T model\n",
    "- `@ml_models_LR_10T` is Linear Regression model on 10T dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "# choose model here\n",
    "model_name = 'ml_models_10T'\n",
    "session.add_import(f\"@{model_name}/model.joblib\")  \n",
    "\n",
    "features = [ 'C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "    import os, joblib\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location=f'@{model_name}', replace=True, name=\"clv_xgboost_udf\")\n",
    "def predict(df:  T.PandasDataFrame[int, str, str, str, str, str, int]) -> T.PandasSeries[float]:\n",
    "    m = read_file('model.joblib')       \n",
    "    df.columns = features\n",
    "    return m.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on data from feature_store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take all training dataset as input, let use smaller dataset in the following\n",
    "inference_df = session.table('feature_store').limit(100)\n",
    "inference_df = inference_df.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "inputs = inference_df.drop(\"TOTAL_SALES\")\n",
    "snowdf_results = inference_df.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('TOTAL_SALES')).alias('ACTUAL_SALES')\n",
    "                    )\n",
    "snowdf_results.write.mode('overwrite').save_as_table('predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on typed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume this is from strealit UI\n",
    "typed_input = [[1969, '66060','M','U','Low Risk','2 yr Degree', 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = session.create_dataframe(typed_input, schema=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "typed_output = input_df.select(*input_df,\n",
    "                    predict(*input_df).alias('PREDICTION'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"C_BIRTH_YEAR\"  |\"CA_ZIP\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_CREDIT_RATING\"  |\"CD_EDUCATION_STATUS\"  |\"CD_DEP_COUNT\"  |\"PREDICTION\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1969            |66060     |M            |U                    |Low Risk            |2 yr Degree            |1               |32340.421875  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "typed_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Customer_Lifetime_Value.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Customer_Lifetime_Value.py\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "st.write('## Introduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pages/CLV_prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pages/CLV_prediction.py\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "st.write('## XGBoost model to predict CLV')\n",
    "\n",
    "st.write('Type inputs here......')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "353961104846001ffa111d7d98923933ef13c251c8e9b3ebc563f652eb6b45f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
